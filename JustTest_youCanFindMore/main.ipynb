{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[0., 0., 0.],\n        [0., 0., 0.]])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "input = torch.empty(2, 3)\n",
    "print(input)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([0, 1, 2, 3, 4])\ntensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\ntensor([1.0000, 2.3000, 3.6000, 4.9000, 6.2000, 7.5000, 8.8000])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "a1 = torch.arange(5)\n",
    "print(a1)\n",
    "a2 = torch.arange(1,10)\n",
    "print(a2)\n",
    "a3 = torch.arange(1,10,1.3)\n",
    "print(a3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "a4 = torch.arange(step=1,start=0,end=17)\n",
    "print(a4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d1bc138cbd34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ma3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: linspace() missing 1 required positional arguments: \"end\""
     ],
     "ename": "TypeError",
     "evalue": "linspace() missing 1 required positional arguments: \"end\"",
     "output_type": "error"
    }
   ],
   "source": [
    "# error in grammar, cause ... \n",
    "# a1 = torch.linspace(5)\n",
    "# print(a1)\n",
    "\n",
    "a2 = torch.linspace(1,10)\n",
    "print(a2)\n",
    "a3 = torch.linspace(1,10,1.3)\n",
    "print(a3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\ntensor([0., 0., 0., 0., 0.])\ntensor([[-3.3907e-16,  3.0705e-41, -6.9469e-17,  3.0705e-41,  1.4013e-45],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  2.3694e-38,  2.3694e-38,  1.1351e-43],\n        [ 0.0000e+00, -3.4444e-16,  3.0705e-41,  2.4043e+31,  4.5583e-41],\n        [ 2.3694e-38,  2.3694e-38,  2.3694e-38,  2.3694e-38,  2.3694e-38]])\ntensor([[                0,    94114104392656,    94114104392640,\n                         0,    94114103951416],\n        [   94114103951424,    94114103951456,                 0,\n                         0,                 0],\n        [                0,                 0,    94114103951480,\n            94114103951488,    94114103951520],\n        [                1,   139712408760384,                 0,\n                         0,                 0],\n        [                0,                 0,   139712408762624,\n         72902014673092609,               256]])\ntensor([[255., 255., 255., 255., 255.],\n        [255., 255., 255., 255., 255.],\n        [255., 255., 255., 255., 255.],\n        [255., 255., 255., 255., 255.],\n        [255., 255., 255., 255., 255.]])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(torch.eye(10)) # default 5*5\n",
    "\n",
    "print(torch.empty(5)) #default 1*5\n",
    "print(torch.empty(5,5))\n",
    "a4 = torch.empty(5,5)\n",
    "print(torch.empty_like(a4, dtype=torch.int64))\n",
    "print(torch.full_like(a4,255))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[ 0.1041, -1.1225, -0.6350],\n        [ 0.5085,  0.3018, -0.4488],\n        [ 0.1041, -1.1225, -0.6350],\n        [ 0.5085,  0.3018, -0.4488],\n        [ 0.1041, -1.1225, -0.6350],\n        [ 0.5085,  0.3018, -0.4488],\n        [ 0.1041, -1.1225, -0.6350],\n        [ 0.5085,  0.3018, -0.4488],\n        [ 0.1041, -1.1225, -0.6350],\n        [ 0.5085,  0.3018, -0.4488]])\ntensor([[ 0.1041, -1.1225, -0.6350],\n        [ 0.5085,  0.3018, -0.4488]])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "x = torch.randn(2,3)\n",
    "aa1 = torch.cat((x,x,x,x,x),0)\n",
    "print(aa1)\n",
    "aa1s = torch.chunk(aa1,5,0)\n",
    "print(aa1s[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "<class 'torchvision.transforms.transforms.Compose'>\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1b1d779fffc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcnt\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PyTorch-YOLOv3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PyTorch-YOLOv3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"KeyError:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Traceback (most recent call last):\n  File \"/home/nvidia/anaconda3/envs/PyTorch-YOLOv3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 99, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/nvidia/anaconda3/envs/PyTorch-YOLOv3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 99, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/nvidia/anaconda3/envs/PyTorch-YOLOv3/lib/python3.7/site-packages/torchvision/datasets/cifar.py\", line 130, in __getitem__\n    target = self.target_transform(target)\n  File \"/home/nvidia/anaconda3/envs/PyTorch-YOLOv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n    img = t(img)\n  File \"/home/nvidia/anaconda3/envs/PyTorch-YOLOv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 92, in __call__\n    return F.to_tensor(pic)\n  File \"/home/nvidia/anaconda3/envs/PyTorch-YOLOv3/lib/python3.7/site-packages/torchvision/transforms/functional.py\", line 50, in to_tensor\n    raise TypeError('pic should be PIL Image or ndarray. Got {}'.format(type(pic)))\nTypeError: pic should be PIL Image or ndarray. Got <class 'int'>\n"
     ],
     "ename": "TypeError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/nvidia/anaconda3/envs/PyTorch-YOLOv3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 99, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/nvidia/anaconda3/envs/PyTorch-YOLOv3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 99, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/nvidia/anaconda3/envs/PyTorch-YOLOv3/lib/python3.7/site-packages/torchvision/datasets/cifar.py\", line 130, in __getitem__\n    target = self.target_transform(target)\n  File \"/home/nvidia/anaconda3/envs/PyTorch-YOLOv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n    img = t(img)\n  File \"/home/nvidia/anaconda3/envs/PyTorch-YOLOv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 92, in __call__\n    return F.to_tensor(pic)\n  File \"/home/nvidia/anaconda3/envs/PyTorch-YOLOv3/lib/python3.7/site-packages/torchvision/transforms/functional.py\", line 50, in to_tensor\n    raise TypeError('pic should be PIL Image or ndarray. Got {}'.format(type(pic)))\nTypeError: pic should be PIL Image or ndarray. Got <class 'int'>\n",
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                               transforms.ToTensor()\n",
    "                               ])\n",
    "\n",
    "train_set = datasets.CIFAR100(root = './',\n",
    "                              train=True,\n",
    "                              transform = transform,\n",
    "                              target_transform=transform,\n",
    "                              download=True)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=2)\n",
    "\n",
    "print(type(transform))\n",
    "to_pil_image = transforms.ToPILImage()\n",
    "\n",
    "cnt = 0\n",
    "for image,label in data_loader:\n",
    "    if cnt>=3:\n",
    "        break\n",
    "    print(label)\n",
    "    img = to_pil_image(image[0])\n",
    "    img.show()\n",
    "    \n",
    "    img = image[0]\n",
    "    img = img.numpy()\n",
    "    img = np.transpose(img,(1, 2, 0))\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "    cnt+=1\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Net(\n  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torch.utils.data\n",
    "show = ToPILImage() # 可以把Tensor转成Image，方便可视化\n",
    "# 第一次运行程序torchvision会自动下载CIFAR-10数据集，\n",
    "# 大约100M，需花费一定的时间，\n",
    "# 如果已经下载有CIFAR-10，可通过root参数指定\n",
    "\n",
    "# 定义对数据的预处理\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(), # 转为Tensor\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # 归一化\n",
    "                             ])\n",
    "\n",
    "# 训练集\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "                    root='./data/', \n",
    "                    train=True, \n",
    "                    download=True,\n",
    "                    transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "                    trainset, \n",
    "                    batch_size=4,\n",
    "                    shuffle=True, \n",
    "                    num_workers=2)\n",
    "\n",
    "# 测试集\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "                    './data/',\n",
    "                    train=False, \n",
    "                    download=True, \n",
    "                    transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "                    testset,\n",
    "                    batch_size=4, \n",
    "                    shuffle=False,\n",
    "                    num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) \n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)  \n",
    "        self.fc1   = nn.Linear(16*5*5, 120)  \n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) \n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2) \n",
    "        x = x.view(x.size()[0], -1) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)        \n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "\n",
    "\n",
    "from torch import optim\n",
    "criterion = nn.CrossEntropyLoss() # 交叉熵损失函数\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) #定义优化器\n",
    "\n",
    "\n",
    "torch.set_num_threads(8)\n",
    "for epoch in range(2):  \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # 输入数据\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()   \n",
    "\n",
    "        # 更新参数 \n",
    "        optimizer.step()\n",
    "\n",
    "        # 打印log信息\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999: # 每2000个batch打印一下训练状态\n",
    "            print('[%d, %5d] loss: %.3f' \\\n",
    "                  % (epoch+1, i+1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next() # 一个batch返回4张图片\n",
    "print('实际的label: ', ' '.join(\\\n",
    "            '%08s'%classes[labels[j]] for j in range(4)))\n",
    "show(torchvision.utils.make_grid(images / 2 - 0.5)).resize((400,100))\n",
    "\n",
    "# 计算图片在每个类别上的分数\n",
    "outputs = net(Variable(images))\n",
    "# 得分最高的那个类\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "print('预测结果: ', ' '.join('%5s'\\\n",
    "            % classes[predicted[j]] for j in range(4)))\n",
    "correct = 0 # 预测正确的图片数\n",
    "total = 0 # 总共的图片数\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('10000张测试集中的准确率为: %d %%' % (100 * correct / total))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "\n",
      "\r 15%|█▌        | 1523712/9912422 [00:14<01:00, 138716.63it/s]",
      "\u001b[A",
      "\n",
      "\r 16%|█▌        | 1540096/9912422 [00:14<00:59, 141118.40it/s]",
      "\u001b[A",
      "\n",
      "\r 16%|█▌        | 1556480/9912422 [00:14<00:58, 142145.63it/s]",
      "\u001b[A",
      "\n",
      "\r 16%|█▌        | 1581056/9912422 [00:14<00:51, 162498.68it/s]",
      "\u001b[A",
      "\n",
      "\r 16%|█▌        | 1605632/9912422 [00:14<00:52, 159676.32it/s]",
      "\u001b[A",
      "\n",
      "\r 16%|█▋        | 1630208/9912422 [00:14<00:52, 156447.32it/s]",
      "\u001b[A",
      "\n",
      "\r 17%|█▋        | 1654784/9912422 [00:14<00:52, 156920.45it/s]",
      "\u001b[A",
      "\n",
      "\r 17%|█▋        | 1671168/9912422 [00:15<00:54, 152550.86it/s]",
      "\u001b[A",
      "\n",
      "\r 17%|█▋        | 1687552/9912422 [00:15<00:56, 145148.30it/s]",
      "\u001b[A",
      "\n",
      "\r 17%|█▋        | 1720320/9912422 [00:15<00:51, 159055.49it/s]",
      "\u001b[A",
      "\n",
      "\r 18%|█▊        | 1744896/9912422 [00:15<00:48, 167050.52it/s]",
      "\u001b[A",
      "\n",
      "\r 18%|█▊        | 1769472/9912422 [00:15<00:45, 180646.86it/s]",
      "\u001b[A",
      "\n",
      "\r 18%|█▊        | 1794048/9912422 [00:15<00:47, 171598.24it/s]",
      "\u001b[A",
      "\n",
      "\r 18%|█▊        | 1818624/9912422 [00:15<00:53, 152107.17it/s]",
      "\u001b[A",
      "\n",
      "\r 19%|█▊        | 1835008/9912422 [00:15<00:51, 155344.91it/s]",
      "\u001b[A",
      "\n",
      "\r 19%|█▊        | 1851392/9912422 [00:16<01:02, 128284.79it/s]",
      "\u001b[A",
      "\n",
      "\r 19%|█▉        | 1867776/9912422 [00:16<01:01, 131521.50it/s]",
      "\u001b[A",
      "\n",
      "\r 19%|█▉        | 1892352/9912422 [00:16<00:59, 134542.13it/s]",
      "\u001b[A",
      "\n",
      "\r 19%|█▉        | 1925120/9912422 [00:16<00:58, 136787.09it/s]",
      "\u001b[A",
      "\n",
      "\r 20%|█▉        | 1957888/9912422 [00:16<00:50, 156320.06it/s]",
      "\u001b[A",
      "\n",
      "\r 20%|█▉        | 1982464/9912422 [00:16<00:46, 170352.20it/s]",
      "\u001b[A",
      "\n",
      "\r 20%|██        | 2007040/9912422 [00:17<00:45, 171922.25it/s]",
      "\u001b[A",
      "\n",
      "\r 21%|██        | 2048000/9912422 [00:17<00:37, 207910.12it/s]",
      "\u001b[A",
      "\n",
      "\r 21%|██        | 2080768/9912422 [00:17<00:34, 226104.57it/s]",
      "\u001b[A",
      "\n",
      "\r 21%|██▏       | 2113536/9912422 [00:17<00:35, 217678.08it/s]",
      "\u001b[A",
      "\n",
      "\r 22%|██▏       | 2138112/9912422 [00:17<00:36, 213537.87it/s]",
      "\u001b[A",
      "\n",
      "\r 22%|██▏       | 2162688/9912422 [00:17<00:35, 218360.32it/s]",
      "\u001b[A",
      "\n",
      "\r 22%|██▏       | 2195456/9912422 [00:17<00:34, 226112.04it/s]",
      "\u001b[A",
      "\n",
      "\r 23%|██▎       | 2236416/9912422 [00:17<00:31, 239901.67it/s]",
      "\u001b[A",
      "\n",
      "\r 23%|██▎       | 2269184/9912422 [00:18<00:39, 195254.91it/s]",
      "\u001b[A",
      "\n",
      "\r 23%|██▎       | 2293760/9912422 [00:18<00:43, 176900.20it/s]",
      "\u001b[A",
      "\n",
      "\r 23%|██▎       | 2318336/9912422 [00:18<00:47, 159401.68it/s]",
      "\u001b[A",
      "\n",
      "\r 24%|██▍       | 2375680/9912422 [00:18<00:41, 180303.07it/s]",
      "\u001b[A",
      "\n",
      "\r 24%|██▍       | 2400256/9912422 [00:18<00:42, 177266.84it/s]",
      "\u001b[A",
      "\n",
      "\r 24%|██▍       | 2424832/9912422 [00:19<00:41, 178561.42it/s]",
      "\u001b[A",
      "\n",
      "\r 25%|██▍       | 2449408/9912422 [00:19<00:41, 179336.73it/s]",
      "\u001b[A",
      "\n",
      "\r 25%|██▍       | 2473984/9912422 [00:19<00:46, 159346.56it/s]",
      "\u001b[A",
      "\n",
      "\r 25%|██▌       | 2498560/9912422 [00:19<00:47, 154557.70it/s]",
      "\u001b[A",
      "\n",
      "\r 25%|██▌       | 2514944/9912422 [00:19<01:01, 120833.05it/s]",
      "\u001b[A",
      "\n",
      "\r 26%|██▌       | 2539520/9912422 [00:19<01:00, 122133.36it/s]",
      "\u001b[A",
      "\n",
      "\r 26%|██▌       | 2555904/9912422 [00:20<00:58, 126114.72it/s]",
      "\u001b[A",
      "\n",
      "\r 26%|██▌       | 2572288/9912422 [00:20<00:54, 135456.15it/s]",
      "\u001b[A",
      "\n",
      "\r 26%|██▌       | 2588672/9912422 [00:20<01:05, 111114.24it/s]",
      "\u001b[A",
      "\n",
      "\r 26%|██▋       | 2605056/9912422 [00:20<01:00, 120469.66it/s]",
      "\u001b[A",
      "\n",
      "\r 26%|██▋       | 2621440/9912422 [00:20<01:10, 103318.87it/s]",
      "\u001b[A",
      "\n",
      "\r 27%|██▋       | 2637824/9912422 [00:20<01:09, 104862.83it/s]",
      "\u001b[A",
      "\n",
      "\r 27%|██▋       | 2654208/9912422 [00:20<01:02, 116841.00it/s]",
      "\u001b[A",
      "\n",
      "\r 27%|██▋       | 2670592/9912422 [00:21<01:03, 114517.75it/s]",
      "\u001b[A",
      "\n",
      "\r 27%|██▋       | 2686976/9912422 [00:21<01:06, 109221.94it/s]",
      "\u001b[A",
      "\n",
      "\r 27%|██▋       | 2703360/9912422 [00:21<01:08, 105088.48it/s]",
      "\u001b[A",
      "\n",
      "\r 27%|██▋       | 2719744/9912422 [00:21<01:06, 108398.42it/s]",
      "\u001b[A",
      "\n",
      "\r 28%|██▊       | 2736128/9912422 [00:21<01:04, 110850.72it/s]",
      "\u001b[A",
      "\n",
      "\r 28%|██▊       | 2752512/9912422 [00:21<01:03, 112594.88it/s]",
      "\u001b[A",
      "\n",
      "\r 28%|██▊       | 2777088/9912422 [00:22<00:58, 122374.99it/s]",
      "\u001b[A",
      "\n",
      "\r 28%|██▊       | 2801664/9912422 [00:22<00:57, 124454.86it/s]",
      "\u001b[A",
      "\n",
      "\r 29%|██▊       | 2826240/9912422 [00:22<00:52, 134141.36it/s]",
      "\u001b[A",
      "\n",
      "\r 29%|██▊       | 2842624/9912422 [00:22<00:52, 134787.54it/s]",
      "\u001b[A",
      "\n",
      "\r 29%|██▉       | 2859008/9912422 [00:22<00:52, 135338.82it/s]",
      "\u001b[A",
      "\n",
      "\r 29%|██▉       | 2875392/9912422 [00:22<00:53, 132417.84it/s]",
      "\u001b[A",
      "\n",
      "\r 29%|██▉       | 2899968/9912422 [00:22<00:51, 135796.27it/s]",
      "\u001b[A",
      "\n",
      "\r 30%|██▉       | 2932736/9912422 [00:23<00:46, 149912.19it/s]",
      "\u001b[A",
      "\n",
      "\r 30%|██▉       | 2949120/9912422 [00:23<00:49, 139976.24it/s]",
      "\u001b[A",
      "\n",
      "\r 30%|██▉       | 2965504/9912422 [00:23<00:56, 123418.66it/s]",
      "\u001b[A",
      "\n",
      "\r 30%|███       | 2990080/9912422 [00:23<00:51, 133328.64it/s]",
      "\u001b[A",
      "\n",
      "\r 30%|███       | 3006464/9912422 [00:23<00:53, 128009.91it/s]",
      "\u001b[A",
      "\n",
      "\r 30%|███       | 3022848/9912422 [00:23<00:58, 118717.75it/s]",
      "\u001b[A",
      "\n",
      "\r 31%|███       | 3039232/9912422 [00:24<01:08, 101001.13it/s]",
      "\u001b[A",
      "\n",
      "\r 31%|███       | 3063808/9912422 [00:24<00:59, 115603.39it/s]",
      "\u001b[A",
      "\n",
      "\r 31%|███       | 3080192/9912422 [00:24<00:57, 118698.32it/s]",
      "\u001b[A",
      "\n",
      "\r 31%|███       | 3096576/9912422 [00:24<00:56, 120067.05it/s]",
      "\u001b[A",
      "\n",
      "\r 31%|███▏      | 3112960/9912422 [00:24<01:02, 109230.17it/s]",
      "\u001b[A",
      "\n",
      "\r 32%|███▏      | 3129344/9912422 [00:24<01:01, 110214.25it/s]",
      "\u001b[A",
      "\n",
      "\r 32%|███▏      | 3145728/9912422 [00:24<01:00, 112058.64it/s]",
      "\u001b[A",
      "\n",
      "\r 32%|███▏      | 3162112/9912422 [00:25<01:16, 88641.77it/s] ",
      "\u001b[A",
      "\n",
      "\r 32%|███▏      | 3194880/9912422 [00:25<01:00, 111797.93it/s]",
      "\u001b[A",
      "\n",
      "\r 32%|███▏      | 3211264/9912422 [00:25<01:02, 107966.10it/s]",
      "\u001b[A",
      "\n",
      "\r 33%|███▎      | 3227648/9912422 [00:25<01:01, 109251.61it/s]",
      "\u001b[A",
      "\n",
      "\r 33%|███▎      | 3244032/9912422 [00:25<00:59, 112431.66it/s]",
      "\u001b[A",
      "\n",
      "\r 33%|███▎      | 3260416/9912422 [00:26<01:23, 79679.43it/s] ",
      "\u001b[A",
      "\n",
      "\r 33%|███▎      | 3301376/9912422 [00:26<01:06, 99006.18it/s]",
      "\u001b[A",
      "\n",
      "\r 33%|███▎      | 3317760/9912422 [00:26<01:05, 99990.76it/s]",
      "\u001b[A",
      "\n",
      "\r 34%|███▎      | 3334144/9912422 [00:26<01:05, 100656.13it/s]",
      "\u001b[A",
      "\n",
      "\r 34%|███▍      | 3350528/9912422 [00:26<01:04, 101322.70it/s]",
      "\u001b[A",
      "\n",
      "\r 34%|███▍      | 3366912/9912422 [00:26<01:03, 103595.87it/s]",
      "\u001b[A",
      "\n",
      "\r 34%|███▍      | 3383296/9912422 [00:27<01:03, 102455.03it/s]",
      "\u001b[A",
      "\n",
      "\r 34%|███▍      | 3399680/9912422 [00:27<00:59, 109288.61it/s]",
      "\u001b[A",
      "\n",
      "\r 34%|███▍      | 3416064/9912422 [00:27<00:56, 115063.11it/s]",
      "\u001b[A",
      "\n",
      "\r 35%|███▍      | 3432448/9912422 [00:27<01:00, 107532.85it/s]",
      "\u001b[A",
      "\n",
      "\r 35%|███▍      | 3448832/9912422 [00:27<01:02, 103943.53it/s]",
      "\u001b[A",
      "\n",
      "\r 35%|███▍      | 3465216/9912422 [00:27<01:02, 102924.96it/s]",
      "\u001b[A",
      "\n",
      "\r 35%|███▌      | 3481600/9912422 [00:28<01:02, 102568.38it/s]",
      "\u001b[A",
      "\n",
      "\r 35%|███▌      | 3497984/9912422 [00:28<01:03, 101475.78it/s]",
      "\u001b[A",
      "\n",
      "\r 35%|███▌      | 3514368/9912422 [00:40<23:50, 4472.12it/s]  ",
      "\u001b[A",
      "\n",
      "\r 36%|███▌      | 3538944/9912422 [00:40<16:48, 6317.83it/s]",
      "\u001b[A",
      "\n",
      "\r 47%|████▋     | 4702208/9912422 [00:40<09:37, 9022.28it/s]",
      "\u001b[A",
      "\n",
      "\r 50%|████▉     | 4939776/9912422 [00:43<06:43, 12336.96it/s]",
      "\u001b[A",
      "\n",
      "\r 52%|█████▏    | 5111808/9912422 [00:46<05:00, 15952.02it/s]",
      "\u001b[A",
      "\n",
      "\r 53%|█████▎    | 5234688/9912422 [00:47<03:39, 21279.19it/s]",
      "\u001b[A",
      "\n",
      "\r 54%|█████▎    | 5324800/9912422 [00:48<02:41, 28429.06it/s]",
      "\u001b[A",
      "\n",
      "\r 54%|█████▍    | 5390336/9912422 [00:49<02:04, 36273.39it/s]",
      "\u001b[A",
      "\n",
      "\r 55%|█████▍    | 5439488/9912422 [00:49<01:42, 43483.20it/s]",
      "\u001b[A",
      "\n",
      "\r 55%|█████▌    | 5480448/9912422 [00:50<01:24, 52243.12it/s]",
      "\u001b[A",
      "\n",
      "\r 56%|█████▌    | 5513216/9912422 [00:50<01:11, 61409.97it/s]",
      "\u001b[A",
      "\n",
      "\r 56%|█████▌    | 5554176/9912422 [00:50<00:54, 80500.11it/s]",
      "\u001b[A",
      "\n",
      "\r 56%|█████▋    | 5586944/9912422 [00:50<00:51, 84666.23it/s]",
      "\u001b[A",
      "\n",
      "\r 57%|█████▋    | 5611520/9912422 [00:51<00:49, 87382.43it/s]",
      "\u001b[A",
      "\n",
      "\r 57%|█████▋    | 5636096/9912422 [00:51<00:45, 92981.66it/s]",
      "\u001b[A",
      "\n",
      "\r 57%|█████▋    | 5652480/9912422 [00:51<00:44, 94854.41it/s]",
      "\u001b[A",
      "\n",
      "\r 57%|█████▋    | 5668864/9912422 [00:51<00:43, 97795.60it/s]",
      "\u001b[A",
      "\n",
      "\r 57%|█████▋    | 5685248/9912422 [00:59<10:26, 6746.24it/s] ",
      "\u001b[A",
      "\n",
      "\r 58%|█████▊    | 5701632/9912422 [00:59<07:35, 9238.76it/s]",
      "\u001b[A",
      "\n",
      "\r 58%|█████▊    | 5718016/9912422 [00:59<05:29, 12741.39it/s]",
      "\u001b[A",
      "\n",
      "\r 58%|█████▊    | 5734400/9912422 [00:59<03:58, 17493.29it/s]",
      "\u001b[A",
      "\n",
      "\r 58%|█████▊    | 5750784/9912422 [01:00<02:54, 23784.19it/s]",
      "\u001b[A",
      "\n",
      "\r 58%|█████▊    | 5767168/9912422 [01:00<02:33, 27003.68it/s]",
      "\u001b[A",
      "\n",
      "\r 58%|█████▊    | 5783552/9912422 [01:00<01:59, 34652.34it/s]",
      "\u001b[A",
      "\n",
      "\r 59%|█████▊    | 5799936/9912422 [01:00<01:32, 44292.68it/s]",
      "\u001b[A",
      "\n",
      "\r 59%|█████▊    | 5816320/9912422 [01:08<10:34, 6454.89it/s] ",
      "\u001b[A",
      "\n",
      "\r 59%|█████▉    | 5857280/9912422 [01:08<07:24, 9122.78it/s]",
      "\u001b[A",
      "\n",
      "\r 59%|█████▉    | 5890048/9912422 [01:08<05:14, 12809.91it/s]",
      "\u001b[A",
      "\n",
      "\r 60%|█████▉    | 5931008/9912422 [01:08<03:46, 17566.25it/s]",
      "\u001b[A",
      "\n",
      "\r 69%|██████▊   | 6799360/9912422 [01:10<02:05, 24835.49it/s]",
      "\u001b[A",
      "\n",
      "\r 69%|██████▉   | 6832128/9912422 [01:10<01:32, 33264.36it/s]",
      "\u001b[A",
      "\n",
      "\r 69%|██████▉   | 6864896/9912422 [01:10<01:12, 41928.09it/s]",
      "\u001b[A",
      "\n",
      "\r 70%|██████▉   | 6889472/9912422 [01:11<01:21, 37220.82it/s]",
      "\u001b[A",
      "\n",
      "\r 70%|███████   | 6946816/9912422 [01:11<00:57, 51245.42it/s]",
      "\u001b[A",
      "\n",
      "\r 71%|███████   | 7004160/9912422 [01:11<00:41, 70501.30it/s]",
      "\u001b[A",
      "\n",
      "\r 71%|███████   | 7045120/9912422 [01:11<00:33, 86015.72it/s]",
      "\u001b[A",
      "\n",
      "\r 72%|███████▏  | 7118848/9912422 [01:12<00:24, 115389.55it/s]",
      "\u001b[A",
      "\n",
      "\r 72%|███████▏  | 7159808/9912422 [01:12<00:20, 136963.21it/s]",
      "\u001b[A",
      "\n",
      "\r 74%|███████▍  | 7380992/9912422 [01:12<00:13, 188583.87it/s]",
      "\u001b[A",
      "\n",
      "\r 75%|███████▌  | 7454720/9912422 [01:13<00:18, 134155.75it/s]",
      "\u001b[A",
      "\n",
      "\r 76%|███████▌  | 7512064/9912422 [01:13<00:16, 147190.64it/s]",
      "\u001b[A",
      "\n",
      "\r 78%|███████▊  | 7684096/9912422 [01:13<00:11, 193082.61it/s]",
      "\u001b[A",
      "\n",
      "\r 78%|███████▊  | 7741440/9912422 [01:13<00:09, 233177.86it/s]",
      "\u001b[A",
      "\n",
      "\r 81%|████████  | 8011776/9912422 [01:14<00:05, 320404.82it/s]",
      "\u001b[A",
      "\n",
      "\r 82%|████████▏ | 8151040/9912422 [01:14<00:04, 414637.00it/s]",
      "\u001b[A",
      "\n",
      "\r 84%|████████▍ | 8323072/9912422 [01:14<00:02, 535966.51it/s]",
      "\u001b[A",
      "\n",
      "\r 87%|████████▋ | 8609792/9912422 [01:14<00:01, 692997.07it/s]",
      "\u001b[A",
      "\n",
      "\r 89%|████████▊ | 8773632/9912422 [01:14<00:01, 800183.78it/s]",
      "\u001b[A",
      "\n",
      "\r 92%|█████████▏| 9076736/9912422 [01:14<00:00, 1014132.15it/s]",
      "\u001b[A",
      "\n",
      "\r 93%|█████████▎| 9265152/9912422 [01:14<00:00, 1087032.51it/s]",
      "\u001b[A",
      "\n",
      "\r 96%|█████████▋| 9560064/9912422 [01:14<00:00, 1319120.17it/s]",
      "\u001b[A",
      "\n",
      "\r 99%|█████████▊| 9764864/9912422 [01:15<00:00, 1311407.24it/s]",
      "\u001b[A",
      "\n\n",
      "\r0it [00:00, ?it/s]",
      "\u001b[A\u001b[A",
      "\n\n",
      "\r  0%|          | 0/28881 [00:00<?, ?it/s]",
      "\u001b[A\u001b[A",
      "\n\n",
      "\r 57%|█████▋    | 16384/28881 [00:00<00:00, 62967.98it/s]",
      "\u001b[A\u001b[A",
      "\n\n",
      "\r32768it [00:00, 41608.11it/s]                           ",
      "\u001b[A\u001b[A",
      "\n\n",
      "\r0it [00:00, ?it/s]",
      "\u001b[A\u001b[A",
      "\n\n",
      "\r  0%|          | 0/1648877 [00:00<?, ?it/s]",
      "\u001b[A\u001b[A",
      "\n\n",
      "\r  1%|          | 16384/1648877 [00:01<00:46, 34896.85it/s]",
      "\u001b[A\u001b[A",
      "\n\n",
      "\r  2%|▏         | 40960/1648877 [00:01<00:39, 40526.83it/s]",
      "\u001b[A\u001b[A",
      "\n\n",
      "\r  6%|▌         | 98304/1648877 [00:01<00:29, 52828.72it/s]",
      "\u001b[A\u001b[A",
      "\n\n",
      "\r 12%|█▏        | 204800/1648877 [00:02<00:20, 70942.60it/s]",
      "\u001b[A\u001b[A",
      "\n\n",
      "\r 25%|██▌       | 417792/1648877 [00:02<00:12, 97000.85it/s]",
      "\u001b[A\u001b[A",
      "\n\n",
      "\r 47%|████▋     | 778240/1648877 [00:02<00:06, 133896.34it/s]",
      "\u001b[A\u001b[A",
      "\n\n",
      "\r 71%|███████   | 1163264/1648877 [00:02<00:02, 182906.36it/s]",
      "\u001b[A\u001b[A",
      "\n\n",
      "\r 95%|█████████▍| 1564672/1648877 [00:03<00:00, 246529.61it/s]",
      "\u001b[A\u001b[A",
      "\n\n\n",
      "\r0it [00:00, ?it/s]",
      "\u001b[A\u001b[A\u001b[A",
      "\n\n\n",
      "\r  0%|          | 0/4542 [00:00<?, ?it/s]",
      "\u001b[A\u001b[A\u001b[A",
      "\n\n\n",
      "\r8192it [00:00, 11286.23it/s]            ",
      "\u001b[A\u001b[A\u001b[A",
      "/home/nvidia/anaconda3/envs/PyTorch-YOLOv3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data\n  warnings.warn(\"train_data has been renamed data\")\n/home/nvidia/anaconda3/envs/PyTorch-YOLOv3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets\n  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/home/nvidia/anaconda3/envs/PyTorch-YOLOv3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:58: UserWarning: test_data has been renamed data\n  warnings.warn(\"test_data has been renamed data\")\n/home/nvidia/anaconda3/envs/PyTorch-YOLOv3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:48: UserWarning: test_labels has been renamed targets\n  warnings.warn(\"test_labels has been renamed targets\")\n",
      "\n",
      "\r9920512it [01:27, 1311407.24it/s]                             ",
      "\u001b[A",
      "\n\n",
      "\r1654784it [00:21, 246529.61it/s]                             ",
      "\u001b[A\u001b[A"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "Extracting ./mnist/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\nProcessing...\nDone!\ntorch.Size([60000, 28, 28])\ntorch.Size([60000])\n",
      "CNN(\n  (conv1): Sequential(\n    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv2): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (out): Linear(in_features=1568, out_features=10, bias=True)\n)\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "View more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "My Youtube Channel: https://www.youtube.com/user/MorvanZhou\n",
    "Dependencies:\n",
    "torch: 0.4\n",
    "torchvision\n",
    "matplotlib\n",
    "\"\"\"\n",
    "# library\n",
    "# standard library\n",
    "import os\n",
    "\n",
    "# third-party library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch.manual_seed(1)    # reproducible\n",
    "\n",
    "# Hyper Parameters\n",
    "EPOCH = 1               # train the training data n times, to save time, we just train 1 epoch\n",
    "BATCH_SIZE = 50\n",
    "LR = 0.001              # learning rate\n",
    "DOWNLOAD_MNIST = False\n",
    "\n",
    "\n",
    "# Mnist digits dataset\n",
    "if not(os.path.exists('./mnist/')) or not os.listdir('./mnist/'):\n",
    "    # not mnist dir or mnist is empyt dir\n",
    "    DOWNLOAD_MNIST = True\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='./mnist/',\n",
    "    train=True,                                     # this is training data\n",
    "    transform=torchvision.transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to\n",
    "                                                    # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]\n",
    "    download=DOWNLOAD_MNIST,\n",
    ")\n",
    "\n",
    "# plot one example\n",
    "print(train_data.train_data.size())                 # (60000, 28, 28)\n",
    "print(train_data.train_labels.size())               # (60000)\n",
    "plt.imshow(train_data.train_data[0].numpy(), cmap='gray')\n",
    "plt.title('%i' % train_data.train_labels[0])\n",
    "plt.show()\n",
    "\n",
    "# Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# pick 2000 samples to speed up testing\n",
    "test_data = torchvision.datasets.MNIST(root='./mnist/', train=False)\n",
    "test_x = torch.unsqueeze(test_data.test_data, dim=1).type(torch.FloatTensor)[:2000]/255.   # shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)\n",
    "test_y = test_data.test_labels[:2000]\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         # input shape (1, 28, 28)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              # input height\n",
    "                out_channels=16,            # n_filters\n",
    "                kernel_size=5,              # filter size\n",
    "                stride=1,                   # filter movement/step\n",
    "                padding=2,                  # if want same width and length of this image after Conv2d, padding=(kernel_size-1)/2 if stride=1\n",
    "            ),                              # output shape (16, 28, 28)\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool2d(kernel_size=2),    # choose max value in 2x2 area, output shape (16, 14, 14)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         # input shape (16, 14, 14)\n",
    "            nn.Conv2d(16, 32, 5, 1, 2),     # output shape (32, 14, 14)\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool2d(2),                # output shape (32, 7, 7)\n",
    "        )\n",
    "        self.out = nn.Linear(32 * 7 * 7, 10)   # fully connected layer, output 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)           # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        output = self.out(x)\n",
    "        return output, x    # return x for visualization\n",
    "\n",
    "\n",
    "cnn = CNN()\n",
    "print(cnn)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted\n",
    "\n",
    "# following function (plot_with_labels) is for visualization, can be ignored if not interested\n",
    "from matplotlib import cm\n",
    "try: from sklearn.manifold import TSNE; HAS_SK = True\n",
    "except: HAS_SK = False; print('Please install sklearn for layer visualization')\n",
    "def plot_with_labels(lowDWeights, labels):\n",
    "    plt.cla()\n",
    "    X, Y = lowDWeights[:, 0], lowDWeights[:, 1]\n",
    "    for x, y, s in zip(X, Y, labels):\n",
    "        c = cm.rainbow(int(255 * s / 9)); plt.text(x, y, s, backgroundcolor=c, fontsize=9)\n",
    "    plt.xlim(X.min(), X.max()); plt.ylim(Y.min(), Y.max()); plt.title('Visualize last layer'); plt.show(); plt.pause(0.01)\n",
    "\n",
    "plt.ion()\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader):   # gives batch data, normalize x when iterate train_loader\n",
    "\n",
    "        output = cnn(b_x)[0]               # cnn output\n",
    "        loss = loss_func(output, b_y)   # cross entropy loss\n",
    "        optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        optimizer.step()                # apply gradients\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            test_output, last_layer = cnn(test_x)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.numpy()\n",
    "            accuracy = float((pred_y == test_y.data.numpy()).astype(int).sum()) / float(test_y.size(0))\n",
    "            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)\n",
    "            if HAS_SK:\n",
    "                # Visualization of trained flatten layer (T-SNE)\n",
    "                tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "                plot_only = 500\n",
    "                low_dim_embs = tsne.fit_transform(last_layer.data.numpy()[:plot_only, :])\n",
    "                labels = test_y.numpy()[:plot_only]\n",
    "                plot_with_labels(low_dim_embs, labels)\n",
    "plt.ioff()\n",
    "\n",
    "# print 10 predictions from test data\n",
    "test_output, _ = cnn(test_x[:10])\n",
    "pred_y = torch.max(test_output, 1)[1].data.numpy()\n",
    "print(pred_y, 'prediction number')\n",
    "print(test_y[:10].numpy(), 'real number')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}